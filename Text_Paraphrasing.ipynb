{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Paraphrasing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq-I_JXhgdbJ",
        "outputId": "4f42eb8f-1eff-4e50-cdbf-6f42eed85d12"
      },
      "source": [
        "! pip install transformers sentencepiece -q"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.9 MB 32.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 63.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LLIawxqgkHK",
        "outputId": "164368b2-75cb-4c01-a1e9-02409bbfadc1"
      },
      "source": [
        "! pip install -q gradio"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0 MB 33.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 206 kB 53.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 38.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 42.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 961 kB 50.1 MB/s \n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flask-cachebuster (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "hs2EJFCGgg11",
        "outputId": "f892a617-9448-44ae-e7b5-a55ec4cfa535"
      },
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)# Diverse Beam search\n",
        "\n",
        "\n",
        "\n",
        "def generate_text(inp):\n",
        "    context = inp\n",
        "    text = \"paraphrase: \"+context + \" </s>\"\n",
        "    encoding = tokenizer.encode_plus(text,max_length =128, padding=True, return_tensors=\"pt\")\n",
        "    input_ids,attention_mask  = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
        "    model.eval()\n",
        "    diverse_beam_outputs = model.generate(\n",
        "        input_ids=input_ids,attention_mask=attention_mask,\n",
        "        max_length=128,\n",
        "        early_stopping=True,\n",
        "        num_beams=5,\n",
        "        num_beam_groups = 5,\n",
        "        num_return_sequences=5,\n",
        "        diversity_penalty = 0.70)\n",
        "    \n",
        "    sent = tokenizer.decode(diverse_beam_outputs[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    return sent\n",
        "        \n",
        "\n",
        "output_text = gr.outputs.Textbox()\n",
        "gr.Interface(generate_text,\"textbox\", output_text).launch()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "This share link will expire in 72 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted\n",
            "Running on External URL: https://36643.gradio.app\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://36643.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7facc6038810>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Flask 'gradio.networking'>,\n",
              " 'http://127.0.0.1:7860/',\n",
              " 'https://36643.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vcUQOgVgoQe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}